{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNTrpiujdG+lPQjsZi+aDkS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilbaks/LLM_start/blob/main/02_Pytorch_Basics/02_pytorch_basics_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Pytorch Basics"
      ],
      "metadata": {
        "id": "7r4_5op0zbKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.Imports and Installs"
      ],
      "metadata": {
        "id": "vwjhN3czzyAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "RbcVj73gzvmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Tensor basics"
      ],
      "metadata": {
        "id": "SvzuaM_Qzi0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything in pytorch is based on Tensor operations.\n",
        "A tensor can have different dimensions\n",
        "so it can be 1d, 2d, or even 3d and higher\n",
        "\n",
        "scalar, vector, matrix, tensor"
      ],
      "metadata": {
        "id": "ptugcXUY0SGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.empty(size): uninitiallized\n",
        "\n"
      ],
      "metadata": {
        "id": "f8g-nH100fMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.empty(0)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5cPvFDk0_XL",
        "outputId": "b41ad829-12b1-4ab6-962a-7d12c79c7b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.empty(1)  # scalar\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoE5-wYPza3w",
        "outputId": "b07f266c-da8c-4747-d262-21fbde5be2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-2.4082e-38])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmWD9cNpyuVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbee957a-a525-4ab8-ac79-2ab6ee0a28c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.7847e-04, 3.1896e-41, 4.0509e-04])\n"
          ]
        }
      ],
      "source": [
        "x = torch.empty(3)  # vector\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.empty(2, 4) # matrix, 2D\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDvycCqB1M9d",
        "outputId": "bde5d271-c1e6-47bd-a33a-b2076856cbda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.0334e-30,  7.0065e-45,  1.0790e-43,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.empty(2,4,6) # tensor, 3D\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9Vhptvy1a62",
        "outputId": "2940c257-bcc5-45b6-bc1f-ff60203dea38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[2.1715e-18, 2.6704e-06, 1.3679e+22, 2.0450e+20, 1.3484e-05,\n",
            "          8.2356e-10],\n",
            "         [6.7204e-07, 4.2886e-08, 4.2056e-05, 2.6102e-09, 1.4580e-19,\n",
            "          1.1495e+24],\n",
            "         [3.0881e+29, 1.5766e-19, 1.8889e+31, 7.2065e+31, 2.8404e+29,\n",
            "          2.3089e-12],\n",
            "         [1.9421e+31, 2.7491e+20, 6.1949e-04, 1.9421e+31, 2.7491e+20,\n",
            "          2.3078e-12]],\n",
            "\n",
            "        [[7.1760e+22, 7.2250e+28, 1.5766e-19, 4.2192e-08, 2.1666e-04,\n",
            "          3.2768e-09],\n",
            "         [8.5493e+20, 1.0386e+21, 2.1458e-07, 4.2252e-05, 8.3385e-10,\n",
            "          2.3053e-12],\n",
            "         [2.6302e+20, 6.1949e-04, 6.4805e-10, 2.5204e-09, 2.5928e-09,\n",
            "          6.9118e-04],\n",
            "         [6.7743e-10, 6.7122e-07, 2.7149e-06, 1.4580e-19, 4.5450e+30,\n",
            "          1.8524e+28]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.rand(size) random numbers from [0, 1]"
      ],
      "metadata": {
        "id": "kYP1ZLiW1vaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3, 5)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIr7yMwg1osf",
        "outputId": "f5fec3ef-853f-47d8-a668-3309ccf8cf07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5742, 0.8729, 0.6403, 0.2695, 0.4463],\n",
            "        [0.8012, 0.3559, 0.3369, 0.0440, 0.6584],\n",
            "        [0.7371, 0.8095, 0.0812, 0.3585, 0.1946]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- torch.zeros(size)\n",
        "- torch.ones(size)"
      ],
      "metadata": {
        "id": "8d9wroDp2Ha6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros(3, 4, 5)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6QxRH3a19mj",
        "outputId": "b9d9ae23-1d0d-40e9-b26d-d44c51c2e7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones (1, 4)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok8ZZclf2fZJ",
        "outputId": "f9ad4c5f-19ab-4a07-b2ff-bc4c2c8318ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- check size x.size()"
      ],
      "metadata": {
        "id": "-7uM3i0Q2oHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros(4, 3, 5)\n",
        "x.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "895lkbnD2k94",
        "outputId": "d4e40cab-8ae8-4d43-9823-6f52aff19a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- check type"
      ],
      "metadata": {
        "id": "d937A3M92_Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWEBtgS32wEJ",
        "outputId": "b12db6eb-c45a-470d-a3ef-ba0360414af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### construct from data"
      ],
      "metadata": {
        "id": "SwGXlbkI3LCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([5.5, 3])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px_uCWjb3Fpj",
        "outputId": "1a663d75-379f-417a-cc5f-60c007d6dbf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5.5000, 3.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### requires grad argument"
      ],
      "metadata": {
        "id": "_JKan65J3ctT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([5.5, 3], requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeY60VdS3UfT",
        "outputId": "e36a8bf5-0284-4212-fe97-9c2ee46bb669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5.5000, 3.0000], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### operations"
      ],
      "metadata": {
        "id": "O1iZveHi3-nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2, 2)\n",
        "y = torch.rand(2, 2)\n",
        "\n",
        "print(x, y)\n",
        "print(\"x + y=\", x + y)\n",
        "print(torch.add(x, y))\n",
        "print(y.add_(x))\n",
        "print(x - y)\n",
        "print(x * y)\n",
        "print(torch.mul(x, y))\n",
        "print(x / y)\n",
        "print( torch.div(x, y))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC5S8H8S31qL",
        "outputId": "2adee054-b614-4c26-d28b-578d4904fe29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8901, 0.2420],\n",
            "        [0.0595, 0.3032]]) tensor([[0.0094, 0.1762],\n",
            "        [0.5787, 0.7729]])\n",
            "x + y= tensor([[0.8995, 0.4181],\n",
            "        [0.6383, 1.0761]])\n",
            "tensor([[0.8995, 0.4181],\n",
            "        [0.6383, 1.0761]])\n",
            "tensor([[0.8995, 0.4181],\n",
            "        [0.6383, 1.0761]])\n",
            "tensor([[-0.0094, -0.1762],\n",
            "        [-0.5787, -0.7729]])\n",
            "tensor([[0.8007, 0.1012],\n",
            "        [0.0380, 0.3263]])\n",
            "tensor([[0.8007, 0.1012],\n",
            "        [0.0380, 0.3263]])\n",
            "tensor([[0.9896, 0.5787],\n",
            "        [0.0933, 0.2817]])\n",
            "tensor([[0.9896, 0.5787],\n",
            "        [0.0933, 0.2817]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PcyTzXqE5OH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### slicing"
      ],
      "metadata": {
        "id": "jpjFU2hw5e0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(5, 3)\n",
        "print(x)\n",
        "print(x[:, 0])  # all rows and 1 column\n",
        "print(x[0, : ])  # row 1 and all columns\n",
        "print(x[0, 0])# element at 1, 1\n",
        "print(x[0, 0].item())  # Get the actual value if only 1 element in your tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQMQWmsB4KvO",
        "outputId": "22c743e7-b17e-4420-d770-a2ed2b2f5c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6004, 0.5449, 0.6169],\n",
            "        [0.3612, 0.0161, 0.1193],\n",
            "        [0.1364, 0.2887, 0.3590],\n",
            "        [0.4113, 0.4350, 0.5988],\n",
            "        [0.6148, 0.8987, 0.5094]])\n",
            "tensor([0.6004, 0.3612, 0.1364, 0.4113, 0.6148])\n",
            "tensor([0.6004, 0.5449, 0.6169])\n",
            "tensor(0.6004)\n",
            "0.6003619432449341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6EtksmJf6h-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reshape"
      ],
      "metadata": {
        "id": "SudSjK6y6yvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(4, 4)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "DsIqzbVs60cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.view(16)  # make in 1 dimension with 16\n",
        "z = x.view(-1, 8)  # make second dimension 8\n",
        "print(y, y.size())\n",
        "print(z, z.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUhVZna-68I7",
        "outputId": "411324b3-217b-4967-a6eb-14c2637f2f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7433, 0.6902, 0.4909, 0.0018, 0.1425, 0.3744, 0.0073, 0.1982, 0.0796,\n",
            "        0.0455, 0.9417, 0.3317, 0.1222, 0.6917, 0.3028, 0.4550]) torch.Size([16])\n",
            "tensor([[0.7433, 0.6902, 0.4909, 0.0018, 0.1425, 0.3744, 0.0073, 0.1982],\n",
            "        [0.0796, 0.0455, 0.9417, 0.3317, 0.1222, 0.6917, 0.3028, 0.4550]]) torch.Size([2, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.dtype, type(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tISF9Yz_8HJu",
        "outputId": "d80c0935-fd84-4708-8bad-7808d9832e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.float32, torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numpy"
      ],
      "metadata": {
        "id": "ixFOXYWb7-OK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch to numpy with .numpy()\n",
        "x = torch.ones(5)\n",
        "print(x)\n",
        "\n",
        "y = x.numpy()\n",
        "print(type(y), y)\n",
        "\n",
        "# Carful: If the Tensor is on the CPU (not the GPU),\n",
        "# both objects will share the same memory location, so changing one\n",
        "# will also change the other\n",
        "x.add_(1)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9xS9ezq72oT",
        "outputId": "49229824-fa73-4891-8585-38439e60070e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "<class 'numpy.ndarray'> [1. 1. 1. 1. 1.]\n",
            "tensor([2., 2., 2., 2., 2.])\n",
            "[2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy to torch with .from_numpy(x)\n",
        "x = np.ones(5)\n",
        "y = torch.from_numpy(x)\n",
        "print(y)\n",
        "\n",
        "# again be careful when modifying\n",
        "x += 1\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA1pHO6r84gL",
        "outputId": "af9787ed-7dcd-4a21-e79a-633b2554fea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xpDxELHf9Yru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tensors on CPU or on GPU"
      ],
      "metadata": {
        "id": "pLrVzyHX9kpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# by default all tensors are created on the CPU,\n",
        "# but you can also move them to the GPU (only if it's available )\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    y = torch.ones_like(x,device=device)\n",
        "    x = x.to(device)\n",
        "    z = x + y\n",
        "    # z = z.numpy() # not possible because numpy cannot handle GPU tenors\n",
        "    # move to CPU again\n",
        "    z.to(\"cpu\")\n",
        "    z = z.numpy()"
      ],
      "metadata": {
        "id": "Z2hg9jaB9rDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "moarOvqN9qOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Autograd"
      ],
      "metadata": {
        "id": "PycXRLy2IXEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. General intutions"
      ],
      "metadata": {
        "id": "kFbiXTdGQKxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "0gJVod6EIaG6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The autograd package provides automatic differentiation\n",
        "# for all operations on Tensors\n",
        "\n",
        "# requires_grad = True -> tracks all operations on the tensor.\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "y = x + 2\n",
        "print(y)\n",
        "y.backward()  # RuntimeError: grad can be implicitly created only for scalar outputs"
      ],
      "metadata": {
        "id": "ojn4wag-IpvX",
        "outputId": "c4681c1c-644d-4edf-f99b-47f982c0dec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.1488,  0.9591,  0.0400], requires_grad=True)\n",
            "tensor([0.8512, 2.9591, 2.0400], grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e8a5df4a52f5>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# RuntimeError: grad can be implicitly created only for scalar outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    118\u001b[0m                         \u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- scalar case"
      ],
      "metadata": {
        "id": "IRrdi69FLspl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "y = x + 2\n",
        "print(y)  # (x0 + 2, x1 + 2 , x2 + 2)\n",
        "y = y.sum()  # (x0 + 2 + x1 + 2 + x2 + 2)\n",
        "print(y)\n",
        "y.backward()\n",
        "print(x.grad)  # dy/dx0 ,  dy/dx1, dy/dx2"
      ],
      "metadata": {
        "id": "Fq64vbPnLqBl",
        "outputId": "88a138af-9e22-4161-a325-363b7404f915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.7094, -1.0397, -1.3205], requires_grad=True)\n",
            "tensor([1.2906, 0.9603, 0.6795], grad_fn=<AddBackward0>)\n",
            "tensor(2.9305, grad_fn=<SumBackward0>)\n",
            "tensor([1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- non-scalar case"
      ],
      "metadata": {
        "id": "sFYesdJRM_tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "print(x.grad)\n",
        "y = x + 2\n",
        "print(y)\n",
        "grad_output = torch.ones(3)\n",
        "print(grad_output)\n",
        "y.backward(grad_output)\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "y = x + 2\n",
        "print(x.grad)  # Provide the gradient argument\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
        "y.backward(v)\n",
        "print(x.grad)\n",
        "\n",
        "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
        "# It computes partial derivates while applying the chain rule"
      ],
      "metadata": {
        "id": "-E3A6VveNF6i",
        "outputId": "95fb81d7-0a9c-47c4-ef54-ee53d70958c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.0923, -0.2658, -0.0159], requires_grad=True)\n",
            "None\n",
            "tensor([3.0924, 1.7342, 1.9841], grad_fn=<AddBackward0>)\n",
            "tensor([1., 1., 1.])\n",
            "None\n",
            "tensor([1.0000e-01, 1.0000e+00, 1.0000e-04])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "y = x + 2\n",
        "print(x.grad)  # Provide the gradient argument\n",
        "y = x + 2\n",
        "print(y)\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
        "y.backward(v)  # dy/dx\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eC6sEidRMTu",
        "outputId": "06a5eb3e-de88-4803-8102-f425a5d429df"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([1.6487, 2.4264, 1.7382], grad_fn=<AddBackward0>)\n",
            "tensor([1.0000e-01, 1.0000e+00, 1.0000e-04])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.Stop a tensor from tracking history:"
      ],
      "metadata": {
        "id": "Ci8mh3kqRnpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For example during our training loop when we want to update our weights\n",
        "# then this update operation should not be part of the gradient computation\n",
        "# - x.requires_grad_(False)\n",
        "# - x.detach()\n",
        "# - wrap in 'with torch.no_grad():'\n",
        "\n",
        "# .requires_grad_(...) changes an existing flag in-place.\n",
        "a = torch.randn(2, 2)\n",
        "print(f\"{a.requires_grad=}\")\n",
        "b = ((a * 3) / (a - 1))\n",
        "print(f\"{b.grad_fn=}\")\n",
        "a.requires_grad_(True)\n",
        "print(f\"{a.requires_grad=}\")\n",
        "b = (a * a).sum()\n",
        "print(f\"{b.grad_fn=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQS-aqW6T7YY",
        "outputId": "dd9d827a-6460-47ef-bf30-e152d72aaaa5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.requires_grad=False\n",
            "b.grad_fn=None\n",
            "a.requires_grad=True\n",
            "b.grad_fn=<SumBackward0 object at 0x796dd85a70d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "print(f\"{a.requires_grad=}\")\n",
        "b = a.detach()\n",
        "print(f\"{b.requires_grad=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe8BBi1VT968",
        "outputId": "1fb1fd51-4fa8-4257-a2cb-87d738a64cbc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.requires_grad=True\n",
            "b.requires_grad=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap in 'with torch.no_grad():'\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "print(f\"{a.requires_grad=}\")\n",
        "with torch.no_grad():\n",
        "    print(f\"{(x ** 2).requires_grad=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLiU4HeFT_3z",
        "outputId": "681f91b0-2268-4762-dd41-a99daedcb939"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.requires_grad=True\n",
            "(x ** 2).requires_grad=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.Zero gradients"
      ],
      "metadata": {
        "id": "7M0uKkeWUCKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# backward() accumulates the gradient for this tensor into .grad attribute.\n",
        "# !!! We need to be careful during optimization !!!\n",
        "# Use .zero_() to empty the gradients before a new optimization step!\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "print(f\"{weights=}\")\n",
        "\n",
        "for epoch in range(3):\n",
        "    # just a dummy example\n",
        "    model_output = (weights*3).sum()\n",
        "    print(f\"{model_output=}\")\n",
        "    model_output.backward()\n",
        "\n",
        "    print(weights.grad)\n",
        "\n",
        "    # optimize model, i.e. adjust weights...\n",
        "    with torch.no_grad():\n",
        "        weights -= 0.1 * weights.grad\n",
        "\n",
        "    # this is important! It affects the final weights & output\n",
        "    weights.grad.zero_()\n",
        "\n",
        "print(f\"{weights=}\")\n",
        "print(f\"{model_output=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0as4DNO8UItG",
        "outputId": "ffbcb0f5-4a43-4a4f-d60b-6be12884d41e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights=tensor([1., 1., 1., 1.], requires_grad=True)\n",
            "model_output=tensor(12., grad_fn=<SumBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n",
            "model_output=tensor(8.4000, grad_fn=<SumBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n",
            "model_output=tensor(4.8000, grad_fn=<SumBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n",
            "weights=tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n",
            "model_output=tensor(4.8000, grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Backpropagation"
      ],
      "metadata": {
        "id": "IXg35BlhZZla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "# This is the parameter we want to optimize -> requires_grad=True\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# forward pass to compute loss\n",
        "y_predicted = w * x\n",
        "loss = (y_predicted - y)**2\n",
        "print(f\"{loss=}\")\n",
        "\n",
        "# backward pass to compute gradient dLoss/dw\n",
        "loss.backward()\n",
        "print(f\"{w.grad=}\")\n",
        "\n",
        "# update weights\n",
        "# next forward and backward pass...\n",
        "\n",
        "# continue optimizing:\n",
        "# update weights, this operation should not be part of the computational graph\n",
        "with torch.no_grad():\n",
        "    w -= 0.01 * w.grad\n",
        "# don't forget to zero the gradients\n",
        "w.grad.zero_()\n",
        "\n",
        "# next forward and backward pass..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC-PcGIBZc8Z",
        "outputId": "5e6be86b-fade-45c8-88ea-3643d6e19182"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss=tensor(1., grad_fn=<PowBackward0>)\n",
            "w.grad=tensor(-2.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Gradient descent"
      ],
      "metadata": {
        "id": "LoKXoXYAl08L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Manually"
      ],
      "metadata": {
        "id": "InLauSnnl7x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute every step manually\n",
        "\n",
        "# Linear regression\n",
        "# f = w * x\n",
        "\n",
        "# here : f = 2 * x\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "print(f\"{X=}\")\n",
        "print(f\"{Y=}\")\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "# J = MSE = 1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N * 2x(w*x - y)\n",
        "def gradient(x, y, y_pred):\n",
        "    return np.mean(2*x*(y_pred - y))\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # calculate gradients\n",
        "    dw = gradient(X, Y, y_pred)\n",
        "\n",
        "    # update weights\n",
        "    w -= learning_rate * dw\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "id": "-dOs6qkUl5qt",
        "outputId": "36d518ad-2335-4a03-cef1-25544cc5f2c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X=array([1., 2., 3., 4.], dtype=float32)\n",
            "Y=array([2., 4., 6., 8.], dtype=float32)\n",
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 3: w = 0.772, loss = 15.66018677\n",
            "epoch 5: w = 1.113, loss = 8.17471600\n",
            "epoch 7: w = 1.359, loss = 4.26725292\n",
            "epoch 9: w = 1.537, loss = 2.22753215\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 13: w = 1.758, loss = 0.60698175\n",
            "epoch 15: w = 1.825, loss = 0.31684822\n",
            "epoch 17: w = 1.874, loss = 0.16539653\n",
            "epoch 19: w = 1.909, loss = 0.08633806\n",
            "Prediction after training: f(5) = 9.612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.Auto"
      ],
      "metadata": {
        "id": "Acu2DhnXsGEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Here we replace the manually computed gradient with autograd\n",
        "\n",
        "# Linear regression\n",
        "# f = w * x\n",
        "\n",
        "# here : f = 2 * x\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    #w.data = w.data - learning_rate * w.grad\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "\n",
        "    # zero the gradients after updating\n",
        "    w.grad.zero_()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "id": "zG3x3IfYsIHQ",
        "outputId": "ce5cc3b2-0a95-4668-d928-0d307c3e00c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Loss and optimizer"
      ],
      "metadata": {
        "id": "o5P9MSDtwD7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Design model (input, output, forward pass with different layers)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#       - Forward = compute prediction and loss\n",
        "#       - Backward = compute gradients\n",
        "#       - Update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Linear regression\n",
        "# f = w * x\n",
        "\n",
        "# here : f = 2 * x\n",
        "\n",
        "# 0) Training samples\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "# 1) Design Model: Weights to optimize and forward function\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "# 2) Define loss and optimizer\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "# callable function\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
        "\n",
        "# 3) Training loop\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_predicted = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_predicted)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero the gradients after updating\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print('epoch ', epoch+1, ': w = ', w, ' loss = ', l)\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "id": "Y1gSYyJcwHaY",
        "outputId": "8788e46d-67a9-4833-f898-c523d8fb5323",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch  1 : w =  tensor(0.3000, requires_grad=True)  loss =  tensor(30., grad_fn=<MseLossBackward0>)\n",
            "epoch  11 : w =  tensor(1.6653, requires_grad=True)  loss =  tensor(1.1628, grad_fn=<MseLossBackward0>)\n",
            "epoch  21 : w =  tensor(1.9341, requires_grad=True)  loss =  tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
            "epoch  31 : w =  tensor(1.9870, requires_grad=True)  loss =  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "epoch  41 : w =  tensor(1.9974, requires_grad=True)  loss =  tensor(6.7705e-05, grad_fn=<MseLossBackward0>)\n",
            "epoch  51 : w =  tensor(1.9995, requires_grad=True)  loss =  tensor(2.6244e-06, grad_fn=<MseLossBackward0>)\n",
            "epoch  61 : w =  tensor(1.9999, requires_grad=True)  loss =  tensor(1.0176e-07, grad_fn=<MseLossBackward0>)\n",
            "epoch  71 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(3.9742e-09, grad_fn=<MseLossBackward0>)\n",
            "epoch  81 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(1.4670e-10, grad_fn=<MseLossBackward0>)\n",
            "epoch  91 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(5.0768e-12, grad_fn=<MseLossBackward0>)\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.Linear regression"
      ],
      "metadata": {
        "id": "6QjnhoBBvNtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "import torch  # Main PyTorch library for tensors\n",
        "import torch.nn as nn  # Neural network module in PyTorch\n",
        "import numpy as np\n",
        "from sklearn import datasets  # To import datasets for regression\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "xEgaQrfVvgkF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Prepare Data\n",
        "\n",
        "# Generate a regression dataset with 100 samples, 1 feature, added noise, and a random state for reproducibility\n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
        "\n",
        "# Convert the numpy arrays into PyTorch tensors and ensure the data type is float32 (suitable for PyTorch models)\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "\n",
        "# Reshape y to make it a column vector\n",
        "y = y.view(y.shape[0], 1)\n",
        "\n",
        "# Determine the number of samples and features from the dataset for later use\n",
        "n_samples, n_features = X.shape\n"
      ],
      "metadata": {
        "id": "pZ0jYN6Vv0PS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Model\n",
        "# Linear model f = wx + b\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)"
      ],
      "metadata": {
        "id": "W9ipt8lgwaLx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Loss and optimizer\n",
        "learning_rate = 0.01\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "RrO4Xtp3wnlZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass and loss\n",
        "    y_predicted = model(X)\n",
        "    loss = criterion(y_predicted, y)\n",
        "\n",
        "    # Backward pass and update\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero grad before new step\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "U6UrvIiQwxu1",
        "outputId": "50cf027a-5edd-468b-c467-e302947d3081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 4065.0732\n",
            "epoch: 20, loss = 2865.2883\n",
            "epoch: 30, loss = 2047.1815\n",
            "epoch: 40, loss = 1489.2163\n",
            "epoch: 50, loss = 1108.5956\n",
            "epoch: 60, loss = 848.9003\n",
            "epoch: 70, loss = 671.6777\n",
            "epoch: 80, loss = 550.7128\n",
            "epoch: 90, loss = 468.1322\n",
            "epoch: 100, loss = 411.7454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "predicted = model(X).detach().numpy()\n",
        "\n",
        "plt.plot(X_numpy, y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sja9jYEAw0Tx",
        "outputId": "0f9780e6-e7f7-4dca-dacf-063b62428011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}